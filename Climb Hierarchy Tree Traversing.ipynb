{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Traversing Mountain Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_climb(page):\n",
    "    # If you a span with class=\"rateYDS\" inside an h3 header, then the page is a climb\n",
    "    # Alternative, on a climb page you will be able to find the words \"You & This Route\" inside a div with ...\n",
    "    # id=\"youContainer\" whereas on an area page you will instead be able ot find the words \"You & This Area\"\n",
    "    return bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_links_down_hierarchy(page):\n",
    "    # The links are found inside of div with id=\"viewerLeftNavColContent\"\n",
    "    # The first div inside this div should have id=\"mpbox########\" where #### will be different for every page\n",
    "    # Every link in id=\"mpbox########\" should be a link going further down the hierarchy with the exception:\n",
    "        # The link associated with <img src=\"/img/up.gif\" .../> should be excluded\n",
    "        # Any links to mountainproject.com/scripts/... should be excluded\n",
    "    return list_of_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    # Beautiful soup (or whatever) code here to get page from url\n",
    "    # Ideally, we will do things such that we pass this data on and don't request the link again to ...\n",
    "    # exact other information from the page\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_number_of_ratings(page):\n",
    "    # Each review on a climb page is in its own table of class=\"comvis\"\n",
    "    # Inside each table are 2 <td> cells; everything we need is in the first; probably best to code this in ...\n",
    "    # because the second is user input and could have weird stuff that throws us off.\n",
    "    # We can get the link to the user by looking for href=\"/u/...\".\n",
    "    # We can get where the user rated the route by see if the <img> with ... \n",
    "    # src=\"http://www.mountainproject.com/img/stars/star_b.gif\" is present in the table.\n",
    "    return number_of_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_climb_info(page):\n",
    "    # Will be copied and adapted from \"Climb Page Scraping.ipynb\"\n",
    "    return climb_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_links_to_user_pages(page):\n",
    "    # The links from get_number_of_ratings above.  *We only want the links to users who actually rated the\n",
    "    # route and didn't just submit a comment with no rating.\n",
    "    return list_of_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_user_info(url):\n",
    "    # Will be copied and adapted from \"User Page Scraping.ipynb\"\n",
    "    return user_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's do the traversing recursively\n",
    "\n",
    "def traverse(url, min_climb_ratings, climbs_info, user_links):\n",
    "    page = scrape(url)\n",
    "    \n",
    "    if !is_climb(page):\n",
    "        links = get_all_links_down_hierarchy(page)\n",
    "        for link in links:\n",
    "            traverse(link, min_climb_ratings)\n",
    "    else:\n",
    "        num_ratings = get_number_of_ratings(page)\n",
    "        if num_ratings >= min_climb_ratings\n",
    "            climbs_info.append(extract_climb_info(page)) # or .update() if we want climbs_info to be a dict\n",
    "            user_links.extend([(link, 1) for link in extract_links_to_user_pages(page)])\n",
    "\n",
    "min_climb_ratings = 10\n",
    "min_user_ratings = 10\n",
    "            \n",
    "climbs_info = [] # or {} if we want climbs_info to be a dict\n",
    "user_links = []\n",
    "top_page = scrape(\"http://www.mountainproject.com/destinations/\")\n",
    "state_links = get_all_links_to_state_pages(top_page)\n",
    "for link in state_links:\n",
    "    traverse(link, min_climb_ratings)\n",
    "    \n",
    "user_links_reduced = map_reduce(user_links) # user_links is now a list of tuples (link_to_user, num_ratings)\n",
    "\n",
    "user_links_filtered = [user[0] for user in user_links_filtered if user[1] >= min_user_ratings]\n",
    "\n",
    "users_info = [] # or {} if we want users_info to be a dict\n",
    "for link in user_links_filtered:\n",
    "    users_info.append(extract_user_info(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The %... is an iPython thing, and is not part of the Python language.\n",
    "# In this case we're just telling the plotting library to draw things on\n",
    "# the notebook, instead of on a separate window.\n",
    "%matplotlib inline\n",
    "# See all the \"as ...\" contructs? They're just aliasing the package names.\n",
    "# That way we can call methods like plt.plot() instead of matplotlib.pyplot.plot().\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# The \"requests\" library makes working with HTTP requests easier\n",
    "# than the built-in urllib libraries.\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query tool from mountain project allows us to traverse the climbs along three dimensions: location (state), type of climb, and page. The url of each query takes the form:\n",
    "\n",
    "\"http://www.mountainproject.com ... &selectedIds=%[state id]&type=[type string] ... &page=[page #]\"\n",
    "\n",
    "Each page contains a table where each row is a different climb, and contains the link to each climb.\n",
    "\n",
    "First, we need to design a state dictionary which maps each state to its state_id, which will allow us to traverse by state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#It is fairly easy to get the state id's from the destinations page.\n",
    "states_req = requests.get(\"http://www.mountainproject.com/destinations/\")\n",
    "states_soup = BeautifulSoup(states_req.text, \"html.parser\")\n",
    "\n",
    "#State information is countained in the table with the following attributes/\n",
    "states_table = states_soup.find(\"table\", attrs = {\"align\":\"center\",\"cellspacing\":\"5\", \"cellpadding\":\"0\"})\n",
    "states_entries = states_table.find_all(\"span\", attrs = {\"class\":\"destArea\"})[:-2]\n",
    "\n",
    "#The state id is at the end of the url associated with the state. states_dict is a dictionary where the keys are the\n",
    "#name of the state and the values are the id values.\n",
    "states_dict = {entry.get_text():entry.find(\"a\").get(\"href\").split('/')[-1] for entry in states_entries}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each page from the query is highly standardized, so a single function can be written to get the url's for each route present on a single page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_route_url_loc(page_soup):\n",
    "    #The second table with the following attributes is the table containing the url's of interest.\n",
    "    table = page_soup.find_all(\"table\", attrs = {\"class\" : \"objectList\"})[1]\n",
    "    rows = table.find_all(\"tr\")\n",
    "    \n",
    "    #Ignore the first row as it is just the column titles.\n",
    "    all_urls = [r.find_all(\"a\") for r in rows[1:]]\n",
    "    \n",
    "    #The first link present will always be the link for the route. The last link will be the most specific location\n",
    "    #associated with that route.\n",
    "    route_urls = ['http://www.mountainproject.com' + u[0].get(\"href\") for u in all_urls]\n",
    "    loc_urls = ['http://www.mountainproject.com' + u[-1].get(\"href\") for u in all_urls]\n",
    "    \n",
    "    #route_ids = [url.split('/')[-1] for url in route_urls]\n",
    "    \n",
    "    return route_urls, loc_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The routes can be queried along 5 different types.\n",
    "types = ['mixed','rock','boulder','ice','aid']\n",
    "\n",
    "#Note, the code below currently only runs for NH.\n",
    "nh_url_start = \"http://www.mountainproject.com/scripts/Search.php?searchType=routeFinder&minVotes=10&selectedIds=%27105872225&type=\"\n",
    "nh_url_end = \"&diffMinrock=1800&diffMinboulder=20000&diffMinaid=70000&diffMinice=30000&diffMinmixed=50000&diffMaxrock=5500&diffMaxboulder=21400&diffMaxaid=75260&diffMaxice=38500&diffMaxmixed=60000&is_trad_climb=1&is_sport_climb=1&is_top_rope=1&stars=0&pitches=0&sort1=area&sort2=quality_stars+desc&page=1\" \n",
    "\n",
    "route_urls = []\n",
    "location_urls = []\n",
    "\n",
    "for t in types:\n",
    "    #Str conctenation is prefered to string interpolation because the strings are long, causing the interpolation to take long.\n",
    "    nh_url = nh_url_start + t + nh_url_end\n",
    "    nh_request = requests.get(nh_url)\n",
    "    nh_soup = BeautifulSoup(nh_request.text, \"html.parser\")\n",
    "    \n",
    "    #The query displays 50 routes/page, so the total number of routes can be used to determine how many pages must be scraped.\n",
    "    #There is an edge case where the limit on number of routes is 1000, which still needs to be dealt with.\n",
    "    num_routes = int(nh_soup.find(\"div\", attrs ={\"id\": \"navBox\"}).get_text().split(' ')[-2])\n",
    "    num_pages = num_routes/50 + 1\n",
    "    \n",
    "    \n",
    "    a,b = get_route_url_loc(nh_soup)\n",
    "    route_urls.extend(a)\n",
    "    location_urls.extend(b)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #If there are more than one pages, then we need to iterate over subsequent pages.\n",
    "    if num_pages > 1:\n",
    "        for i in range(2,num_pages + 1):\n",
    "            iter_url = nh_url[:-1] + str(i) #Replace the page number at the end of the url with the next one.\n",
    "            \n",
    "            iter_request = requests.get(iter_url)\n",
    "            iter_soup = BeautifulSoup(iter_request.text, \"html.parser\")\n",
    "            \n",
    "            a,b = get_route_url_loc(iter_soup)\n",
    "            route_urls.extend(a)\n",
    "            location_urls.extend(b)\n",
    "            \n",
    "            time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_location(url):\n",
    "    numerical_coords = (0,0)\n",
    "    loc_req = requests.get(url)\n",
    "    loc_soup = BeautifulSoup(loc_req.text, \"html.parser\")\n",
    "    \n",
    "    #This brings up the table near the top of the page that has location information, if available\n",
    "    loc_table = loc_soup.find(\"div\", attrs ={\"class\":\"rspCol\", \"style\":\"max-width:500px;\"})\n",
    "    #The most direct way to find the location is with the url's: a location is always followed by a map url\n",
    "    table_urls = loc_table.find_all(\"a\", attrs ={\"target\":\"_blank\"})\n",
    "    \n",
    "    for u in table_urls:\n",
    "        #Each map url is labeled by View Map, which allows us to select the location row.\n",
    "        if u.get_text() == 'View Map':\n",
    "            #The most direct way to get the coordinates of the location is by using the coordinates present in the url.\n",
    "            #To access these locations, the url string is split - This splitting is consistent across all map url's.\n",
    "            coords = u.get(\"href\").split('=')[1].split('&')[0].split(',')\n",
    "            numerical_coords = (float(coords[0]), float(coords[1])) #convert unicode coordinates to floats in a tuple.\n",
    "    time.sleep(1)        \n",
    "    return numerical_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat_long = [get_location(loc) for loc in location_urls[0:120]] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'http://www.mountainproject.com/v/the-parking-lot-wall/105898430',\n",
       " u'http://www.mountainproject.com/v/the-prudential/105907474',\n",
       " u'http://www.mountainproject.com/v/the-prudential/105907474',\n",
       " u'http://www.mountainproject.com/v/the-prudential/105907474',\n",
       " u'http://www.mountainproject.com/v/the-prudential/105907474',\n",
       " u'http://www.mountainproject.com/v/no-money-down-left/105907981',\n",
       " u'http://www.mountainproject.com/v/no-money-down-left/105907981',\n",
       " u'http://www.mountainproject.com/v/no-money-down-left/105907981',\n",
       " u'http://www.mountainproject.com/v/no-money-down-left/105907981',\n",
       " u'http://www.mountainproject.com/v/no-money-down-left/105907981',\n",
       " u'http://www.mountainproject.com/v/no-money-down-left/105907981',\n",
       " u'http://www.mountainproject.com/v/no-money-down-left/105907981']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_urls[108:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(location_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc_url = location_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loc_req = requests.get(loc_url)\n",
    "loc_soup = BeautifulSoup(loc_req.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_table = loc_soup.find_all(\"table\", attrs ={\"cellpadding\":\"0\",\"cellspacing\":\"0\"} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc_table = loc_soup.find(\"div\", attrs ={\"class\":\"rspCol\", \"style\":\"max-width:500px;\"})\n",
    "urls = loc_table.find_all(\"a\", attrs ={\"target\":\"_blank\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for u in urls:\n",
    "    if u.get_text() == 'View Map':\n",
    "        coords = u.get(\"href\").split('=')[1].split('&')[0].split(',')\n",
    "        numerical_coords = [float(x) for x in coords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44.15516, -71.36736]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
