{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Traversing Mountain Project For Climbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Extracting State Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query tool from mountain project allows us to traverse the climbs along three dimensions: location (state), type of climb, and page. The url of each query takes the form:\n",
    "\n",
    "\"http://www.mountainproject.com ... &selectedIds=%[state id]&type=[type string] ... &page=[page #]\"\n",
    "\n",
    "Each page contains a table where each row is a different climb, and contains the link to each climb.\n",
    "\n",
    "First, we need to design a state dictionary which maps each state to its state_id, which will allow us to traverse by state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#It is fairly easy to get the state id's from the destinations page.\n",
    "states_req = requests.get(\"http://www.mountainproject.com/destinations/\")\n",
    "states_soup = BeautifulSoup(states_req.text, \"html.parser\")\n",
    "\n",
    "#State information is countained in the table with the following attributes:\n",
    "states_table = states_soup.find(\"table\", attrs = {\"align\":\"center\",\"cellspacing\":\"5\", \"cellpadding\":\"0\"})\n",
    "states_entries = states_table.find_all(\"span\", attrs = {\"class\":\"destArea\"})[:-2]\n",
    "\n",
    "#The state id is at the end of the url associated with the state. states_dict is a dictionary where the keys are the\n",
    "#name of the state and the values are the id values.\n",
    "states_dict = {entry.get_text():entry.find(\"a\").get(\"href\").split('/')[-1] for entry in states_entries}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each page from the query is highly standardized, so a single function can be written to get the url's for each route present on a single page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Getting route urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The route urls will be fetched and put into a dataframe where every row is a url. These url's can be passed into notebook 2 for further data scraping of the quality ratings and user information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_route_url_loc(page_soup, state):\n",
    "    #The second table with the following attributes is the table containing the url's of interest.\n",
    "    table = page_soup.find_all(\"table\", attrs = {\"class\" : \"objectList\"})[1]\n",
    "    rows = table.find_all(\"tr\")\n",
    "    \n",
    "    #Ignore the first row as it is just the column titles.\n",
    "    all_urls = [r.find_all(\"a\") for r in rows[1:]]\n",
    "    \n",
    "    #The first link present will always be the link for the route. The last link will be the most specific location\n",
    "    #associated with that route.\n",
    "    route_urls = ['http://www.mountainproject.com' + u[0].get(\"href\") for u in all_urls]\n",
    "    loc_urls = ['http://www.mountainproject.com' + u[-1].get(\"href\") for u in all_urls]\n",
    "    \n",
    "    state_list = [state] * len(route_urls)\n",
    "    #route_ids = [url.split('/')[-1] for url in route_urls]\n",
    "    \n",
    "    return route_urls, loc_urls, state_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "#The routes can be queried along 5 different types.\n",
    "types = ['mixed','rock','boulder','ice','aid']\n",
    "\n",
    "url_start = \"http://www.mountainproject.com/scripts/Search.php?searchType=routeFinder&minVotes=10&selectedIds=%27\"\n",
    "url_end = \"&diffMinrock=1800&diffMinboulder=20000&diffMinaid=70000&diffMinice=30000&diffMinmixed=50000&diffMaxrock=5500&diffMaxboulder=21400&diffMaxaid=75260&diffMaxice=38500&diffMaxmixed=60000&is_trad_climb=1&is_sport_climb=1&is_top_rope=1&stars=0&pitches=0&sort1=area&sort2=quality_stars+desc&page=1\" \n",
    "\n",
    "route_urls = []\n",
    "location_urls = []\n",
    "large_states = {}\n",
    "state_list = []\n",
    "\n",
    "expected_length = 0\n",
    "\n",
    "#Iterate over each state and each type of climb.\n",
    "for state_name, state_id in states_dict.iteritems():\n",
    "    for t in types:\n",
    "        #Str conctenation is prefered to string interpolation because the strings are long, causing the interpolation to take long.\n",
    "        \n",
    "        url = url_start + state_id + \"&type=\" + t + url_end\n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.text, \"html.parser\")\n",
    "\n",
    "        #The query displays 50 routes/page, so the total number of routes can be used to determine how many pages must be scraped.\n",
    "        num_routes_str = soup.find(\"div\", attrs ={\"id\": \"navBox\"}).get_text().split(' ')[-2]\n",
    "        \n",
    "        #There is an edge case where the limit on number of routes is 1000. This is stored in a dictionary to be dealt with later in this notebook.\n",
    "        if num_routes_str == \"1,000\":\n",
    "            large_states[state_name] = (state_id, t)\n",
    "        else:\n",
    "            num_routes = int(num_routes_str)\n",
    "            expected_length += num_routes\n",
    "            \n",
    "            #print \"Number of routes is: \" + str(num_routes)\n",
    "            \n",
    "            if num_routes > 0:\n",
    "                num_pages = int(math.ceil(float(num_routes)/50.0))\n",
    "                \n",
    "                #print \"num of pages is: \" + str(num_pages)\n",
    "                a,b,c = get_route_url_loc(soup, state_name)\n",
    "                \n",
    "                #print \"length of route urls page 1 is: \" + str(len(a))\n",
    "                \n",
    "                route_urls.extend(a)\n",
    "                location_urls.extend(b)\n",
    "                state_list.extend(c)\n",
    "                \n",
    "                time.sleep(1)\n",
    "\n",
    "                #If there are more than one pages, then we need to iterate over subsequent pages.\n",
    "                if num_pages > 1:\n",
    "                    for i in range(2,num_pages + 1):\n",
    "                        iter_url = url[:-1] + str(i) #Replace the page number at the end of the url with the next one.\n",
    "\n",
    "                        iter_request = requests.get(iter_url)\n",
    "                        iter_soup = BeautifulSoup(iter_request.text, \"html.parser\")\n",
    "\n",
    "                        a,b,c = get_route_url_loc(iter_soup, state_name)\n",
    "                        \n",
    "                        #print \"length of route urls page\"+ str(i) + \" is: \" + str(len(a))\n",
    "                        \n",
    "                        route_urls.extend(a)\n",
    "                        location_urls.extend(b)\n",
    "                        state_list.extend(c)\n",
    "\n",
    "                        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9595"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(route_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9595"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9595"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that all the available routes were successfully placed in the correct lists since they all contain the same lengths. The length of the state_list matches the length of the route urls, indicating that the states were successfully stored for each route which will alow for state-based identification of different routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'California': (u'105708959', 'rock'), u'Arizona': (u'105708962', 'rock'), u'Utah': (u'105708957', 'rock'), u'Colorado': (u'105708956', 'rock')}\n"
     ]
    }
   ],
   "source": [
    "print large_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are still routes missing for 4 \"large states\" whose query reached the limit of 1,000 routes. All of the large results were associeted with rock climbs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Extracting States Above the Query Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The routes on the Mountain Climb website are organized into a hierarchy. In order to bypass the query results limit, the code will dive down the hierarchy and be run on a per-area basis, where the area is the sublevel below the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def area_url_state(state):\n",
    "    state_url = \"http://mountainproject.com/v/\" + state.lower() +'/' + large_states[state][0]\n",
    "    state_req = requests.get(state_url)\n",
    "    state_soup = BeautifulSoup(state_req.text,\"html.parser\")\n",
    "    \n",
    "    #This table includes all the areas found in a state\n",
    "    area_table = state_soup.find(\"div\", attrs ={\"class\": \"roundedTop\"})\n",
    "    \n",
    "    #The first and last two \"a\" html tags are not areas. They are related to a drop down menu, and google maps links\n",
    "    #at the beginning and end respecitvley. These entries can be dropped safely due to the uniformity in page \n",
    "    list_of_areas = area_table.find_all(\"a\")[1:-2]\n",
    "    area_url = [x.get(\"href\") for x in list_of_areas]\n",
    "    area_id = [x.split('/')[-1] for x in area_url]\n",
    "    \n",
    "    return area_url, area_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_start = \"http://www.mountainproject.com/scripts/Search.php?searchType=routeFinder&minVotes=10&selectedIds=%27\"\n",
    "url_end = \"&type=rock&diffMinrock=1800&diffMinboulder=20000&diffMinaid=70000&diffMinice=30000&diffMinmixed=50000&diffMaxrock=5500&diffMaxboulder=21400&diffMaxaid=75260&diffMaxice=38500&diffMaxmixed=60000&is_trad_climb=1&is_sport_climb=1&is_top_rope=1&stars=0&pitches=0&sort1=area&sort2=quality_stars+desc&page=1\" \n",
    "\n",
    "area_route_urls = []\n",
    "area_location_urls = []\n",
    "large_areas = {}\n",
    "area_state_list = []\n",
    "\n",
    "#expected length is a debugging variable to keep track of what the array size should be at the end of running this code.\n",
    "expected_length_area = 0\n",
    "\n",
    "for state in large_states:\n",
    "    \n",
    "    area_url_list, area_id_list = area_url_state(state)\n",
    "    \n",
    "    #Now the iteration runs only over area and not type, since all the missing climbs are rock type.\n",
    "    for area_id in area_id_list:\n",
    "        area_query_url = url_start + area_id + url_end\n",
    "        area_req = requests.get(area_query_url)\n",
    "        area_soup = BeautifulSoup(area_req.text, \"html.parser\")\n",
    "        \n",
    "        #The query displays 50 routes/page, so the total number of routes can be used to determine how many pages must be scraped.\n",
    "        #There is an edge case where the limit on number of routes is 1000, which still needs to be dealt with.\n",
    "        num_routes_str = area_soup.find(\"div\", attrs ={\"id\": \"navBox\"}).get_text().split(' ')[-2]\n",
    "        \n",
    "        #As a precaution, the same large query edge case is included to determine if further traversing down the hierarchy will be needed.\n",
    "        if num_routes_str == \"1,000\":\n",
    "            large_areas[area_id] = state\n",
    "        else:\n",
    "            num_routes = int(num_routes_str)\n",
    "            expected_length_area += num_routes\n",
    "            \n",
    "            if num_routes > 0:\n",
    "                num_pages = int(math.ceil(float(num_routes)/50.0))\n",
    "\n",
    "                a,b,c = get_route_url_loc(area_soup, state)\n",
    "                area_route_urls.extend(a)\n",
    "                area_location_urls.extend(b)\n",
    "                area_state_list.extend(c)\n",
    "                \n",
    "                time.sleep(1)\n",
    "\n",
    "                #If there are more than one pages, then we need to iterate over subsequent pages.\n",
    "                if num_pages > 1:\n",
    "                    for i in range(2,num_pages + 1):\n",
    "                        iter_url = area_query_url[:-1] + str(i) #Replace the page number at the end of the url with the next one.\n",
    "\n",
    "                        iter_request = requests.get(iter_url)\n",
    "                        iter_soup = BeautifulSoup(iter_request.text, \"html.parser\")\n",
    "\n",
    "                        a,b,c = get_route_url_loc(iter_soup, state)\n",
    "                        area_route_urls.extend(a)\n",
    "                        area_location_urls.extend(b)\n",
    "                        area_state_list.extend(c)\n",
    "\n",
    "                        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8300"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(area_route_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8300"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_length_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the code got all the expected routes based on the length of the output array. Additionally, we can see that without the edge case many routes would have been, these four \"large\" states contribute almost as many routes on their own as all the other \"smaller\" states. If the limiting edge case had not been caught, these routes would only have contributed 4,000 routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'105739213': u'Utah'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the states includes large sub areas. This is linked to an identifying sub-area id, which can be put in the url for further traversal to the next sub-area level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down the Hierarchy Once More: Large Sub-Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below runs very similarly to all the code above for similar scraping, just one level further down the hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_state = large_areas.values()[0]\n",
    "sub_area_url = \"http://www.mountainproject.com/v/wasatch-range/105739213\"\n",
    "\n",
    "sub_area_req = requests.get(sub_area_url)\n",
    "sub_area_soup = BeautifulSoup(sub_area_req.text,\"html.parser\")\n",
    "sub_area_table = sub_area_soup.find(\"div\", attrs ={\"class\": \"roundedTop\"})\n",
    "list_of_sub_areas = sub_area_table.find_all(\"a\")[1:-2]\n",
    "    \n",
    "sub_area_url = [x.get(\"href\") for x in list_of_sub_areas]\n",
    "sub_area_id = [x.split('/')[-1] for x in sub_area_url]\n",
    "\n",
    "\n",
    "url_start = \"http://www.mountainproject.com/scripts/Search.php?searchType=routeFinder&minVotes=10&selectedIds=%27\"\n",
    "url_end = \"&type=rock&diffMinrock=1800&diffMinboulder=20000&diffMinaid=70000&diffMinice=30000&diffMinmixed=50000&diffMaxrock=5500&diffMaxboulder=21400&diffMaxaid=75260&diffMaxice=38500&diffMaxmixed=60000&is_trad_climb=1&is_sport_climb=1&is_top_rope=1&stars=0&pitches=0&sort1=area&sort2=quality_stars+desc&page=1\" \n",
    "\n",
    "sub_area_route_urls = []\n",
    "sub_area_location_urls = []\n",
    "large_sub_areas = {}\n",
    "sub_area_state_list = []\n",
    "\n",
    "expected_length_sub_area = 0\n",
    "\n",
    "for area_id in sub_area_id:\n",
    "    area_query_url = url_start + area_id + url_end\n",
    "\n",
    "    area_req = requests.get(area_query_url)\n",
    "    area_soup = BeautifulSoup(area_req.text, \"html.parser\")\n",
    "\n",
    "    #The query displays 50 routes/page, so the total number of routes can be used to determine how many pages must be scraped.\n",
    "    #There is an edge case where the limit on number of routes is 1000, which still needs to be dealt with.\n",
    "    num_routes_str = area_soup.find(\"div\", attrs ={\"id\": \"navBox\"}).get_text().split(' ')[-2]\n",
    "    \n",
    "    if num_routes_str == \"1,000\":\n",
    "        large_sub_areas[area_id] = state\n",
    "    else:\n",
    "        num_routes = int(num_routes_str)\n",
    "        expected_length_sub_area += num_routes\n",
    "        \n",
    "        if num_routes > 0:\n",
    "            num_pages = int(math.ceil(float(num_routes)/50.0))\n",
    "\n",
    "            a,b,c = get_route_url_loc(area_soup, sub_state)\n",
    "            sub_area_route_urls.extend(a)\n",
    "            sub_area_location_urls.extend(b)\n",
    "            sub_area_state_list.extend(c)\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "            #If there are more than one pages, then we need to iterate over subsequent pages.\n",
    "            if num_pages > 1:\n",
    "                for i in range(2,num_pages + 1):\n",
    "                    iter_url = area_query_url[:-1] + str(i) #Replace the page number at the end of the url with the next one.\n",
    "\n",
    "                    iter_request = requests.get(iter_url)\n",
    "                    iter_soup = BeautifulSoup(iter_request.text, \"html.parser\")\n",
    "\n",
    "                    a,b,c = get_route_url_loc(iter_soup, sub_state)\n",
    "                    sub_area_route_urls.extend(a)\n",
    "                    sub_area_location_urls.extend(b)\n",
    "                    sub_area_state_list.extend(c)\n",
    "\n",
    "                    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sub_area_route_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_length_sub_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, all the route urls are fully scrapped. The sub-area was just above the query limit of 1,000. There were no sub-areas of length over 1,000 at this level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18901\n",
      "18901\n",
      "18901\n"
     ]
    }
   ],
   "source": [
    "# All the different lists are concatenated together.\n",
    "all_route_urls = route_urls + area_route_urls + sub_area_route_urls\n",
    "all_location_urls = location_urls + area_location_urls + sub_area_location_urls\n",
    "all_states = state_list + area_state_list + sub_area_state_list\n",
    "\n",
    "print len(all_route_urls)\n",
    "print len(all_location_urls)\n",
    "print len(all_states)\n",
    "\n",
    "#routes_dict = {'route_url': route_urls, 'location_url': location_urls, 'state':state_list}\n",
    "#df_routes = pd.DataFrame(routes_dict)\n",
    "#df_routes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Creating DataFrames and Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_url</th>\n",
       "      <th>route_url</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/the-hobbit/10...</td>\n",
       "      <td>Oklahoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/accidents-wil...</td>\n",
       "      <td>Oklahoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/super-slide/1...</td>\n",
       "      <td>Oklahoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/whos-got-the-...</td>\n",
       "      <td>Oklahoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.mountainproject.com/v/headwall/1058...</td>\n",
       "      <td>http://www.mountainproject.com/v/last-of-the-g...</td>\n",
       "      <td>Oklahoma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        location_url                                          route_url     state\n",
       "0  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/the-hobbit/10...  Oklahoma\n",
       "1  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/accidents-wil...  Oklahoma\n",
       "2  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/super-slide/1...  Oklahoma\n",
       "3  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/whos-got-the-...  Oklahoma\n",
       "4  http://www.mountainproject.com/v/headwall/1058...  http://www.mountainproject.com/v/last-of-the-g...  Oklahoma"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All of the data is placed into a dataframe.\n",
    "routes_dict = {'route_url': all_route_urls, 'location_url': all_location_urls, 'state': all_states}\n",
    "df_routes = pd.DataFrame(routes_dict)\n",
    "df_routes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_url</th>\n",
       "      <th>route_url</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2416</th>\n",
       "      <td>http://www.mountainproject.com/v/chaos-boulder...</td>\n",
       "      <td>http://www.mountainproject.com/v/circus-trick/...</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2417</th>\n",
       "      <td>http://www.mountainproject.com/v/the-black-box...</td>\n",
       "      <td>http://www.mountainproject.com/v/arete-left-si...</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>http://www.mountainproject.com/v/the-black-box...</td>\n",
       "      <td>http://www.mountainproject.com/v/arete-right-s...</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2419</th>\n",
       "      <td>http://www.mountainproject.com/v/the-black-box...</td>\n",
       "      <td>http://www.mountainproject.com/v/slots-of-fun/...</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2420</th>\n",
       "      <td>http://www.mountainproject.com/v/the-black-box...</td>\n",
       "      <td>http://www.mountainproject.com/v/center-start/...</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2421</th>\n",
       "      <td>http://www.mountainproject.com/v/flat-top-boul...</td>\n",
       "      <td>http://www.mountainproject.com/v/silly-wabbit/...</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2422</th>\n",
       "      <td>http://www.mountainproject.com/v/flat-top-boul...</td>\n",
       "      <td>http://www.mountainproject.com/v/the-pregnancy...</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2423</th>\n",
       "      <td>http://www.mountainproject.com/v/flat-top-boul...</td>\n",
       "      <td>http://www.mountainproject.com/v/mr-trujillos-...</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>http://www.mountainproject.com/v/flat-top-boul...</td>\n",
       "      <td>http://www.mountainproject.com/v/leftover-love...</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>http://www.mountainproject.com/v/flat-top-boul...</td>\n",
       "      <td>http://www.mountainproject.com/v/kick-start-lo...</td>\n",
       "      <td>Utah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           location_url                                          route_url state\n",
       "2416  http://www.mountainproject.com/v/chaos-boulder...  http://www.mountainproject.com/v/circus-trick/...  Utah\n",
       "2417  http://www.mountainproject.com/v/the-black-box...  http://www.mountainproject.com/v/arete-left-si...  Utah\n",
       "2418  http://www.mountainproject.com/v/the-black-box...  http://www.mountainproject.com/v/arete-right-s...  Utah\n",
       "2419  http://www.mountainproject.com/v/the-black-box...  http://www.mountainproject.com/v/slots-of-fun/...  Utah\n",
       "2420  http://www.mountainproject.com/v/the-black-box...  http://www.mountainproject.com/v/center-start/...  Utah\n",
       "2421  http://www.mountainproject.com/v/flat-top-boul...  http://www.mountainproject.com/v/silly-wabbit/...  Utah\n",
       "2422  http://www.mountainproject.com/v/flat-top-boul...  http://www.mountainproject.com/v/the-pregnancy...  Utah\n",
       "2423  http://www.mountainproject.com/v/flat-top-boul...  http://www.mountainproject.com/v/mr-trujillos-...  Utah\n",
       "2424  http://www.mountainproject.com/v/flat-top-boul...  http://www.mountainproject.com/v/leftover-love...  Utah\n",
       "2425  http://www.mountainproject.com/v/flat-top-boul...  http://www.mountainproject.com/v/kick-start-lo...  Utah"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One of the large states, Utah, is accounted for.\n",
    "df_routes[df_routes['state'] == 'Utah'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All of the routes data from scrapping are saved to a csv to avoid\n",
    "df_routes.to_csv(\"df_routes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>location_url</th>\n",
       "      <th>route_url</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/the-hobbit/10...</td>\n",
       "      <td>Oklahoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/accidents-wil...</td>\n",
       "      <td>Oklahoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/super-slide/1...</td>\n",
       "      <td>Oklahoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/whos-got-the-...</td>\n",
       "      <td>Oklahoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.mountainproject.com/v/headwall/1058...</td>\n",
       "      <td>http://www.mountainproject.com/v/last-of-the-g...</td>\n",
       "      <td>Oklahoma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       location_url                                          route_url     state\n",
       "0           0  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/the-hobbit/10...  Oklahoma\n",
       "1           1  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/accidents-wil...  Oklahoma\n",
       "2           2  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/super-slide/1...  Oklahoma\n",
       "3           3  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/whos-got-the-...  Oklahoma\n",
       "4           4  http://www.mountainproject.com/v/headwall/1058...  http://www.mountainproject.com/v/last-of-the-g...  Oklahoma"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_routes = pd.read_csv(\"df_routes.csv\")\n",
    "df_routes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Scraping Geographic Data: Latitude and Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make a map showing climb destinations, we wish to get the coordinates of the climbs. Climb pages themselves do not have the information, but each climb page has a navigation box, which allows one to see the area hierarchy leading to the climb. This will be exploited, as the area pages do contain the latitude/longitude coordinates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_location(url):\n",
    "    numerical_coords = [0,0]\n",
    "    loc_req = requests.get(url)\n",
    "    loc_soup = BeautifulSoup(loc_req.text, \"html.parser\")\n",
    "    \n",
    "    #This brings up the table near the top of the page that has location information, if available\n",
    "    loc_table = loc_soup.find(\"div\", attrs ={\"class\":\"rspCol\", \"style\":\"max-width:500px;\"})\n",
    "    #The most direct way to find the location is with the url's: a location is always followed by a map url\n",
    "    table_urls = loc_table.find_all(\"a\", attrs ={\"target\":\"_blank\"})\n",
    "    \n",
    "    for u in table_urls:\n",
    "        #Each map url is labeled by View Map, which allows us to select the location row.\n",
    "        if u.get_text() == 'View Map':\n",
    "            #The most direct way to get the coordinates of the location is by using the coordinates present in the url.\n",
    "            #To access these locations, the url string is split - This splitting is consistent across all map url's.\n",
    "            coords = u.get(\"href\").split('=')[1].split('&')[0].split(',')\n",
    "            numerical_coords = (float(coords[0]), float(coords[1])) #convert unicode coordinates to floats in a tuple.\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    #Some very specific area pages lack latitude/longitude data. As such, we can check against the default value.\n",
    "    #If the default value persists, we can proceed one level up the area hierarchy and attempt to find the coordiantes.\n",
    "    if numerical_coords == [0,0]:\n",
    "        \n",
    "        #The nav box object contains the hierarchy of subareas. The subareas go from least specific to most specific\n",
    "        less_specific_area = loc_soup.find(\"div\", attrs ={\"id\":\"navBox\"}).find_all(\"a\")\n",
    "        less_specific_url = [x.get(\"href\") for x in less_specific_area]\n",
    "        i= 1\n",
    "        n = len(less_specific_url)\n",
    "        \n",
    "        # The while loop it\n",
    "        while numerical_coords == [0,0] and i < n:\n",
    "            #Negative indexing as the goal of the loop is to traverse up the hierarchy to find the most specific\n",
    "            #coordinate available.\n",
    "            numerical_coords = get_location(\"http://www.mountainproject.com\" + less_specific_url[-i])\n",
    "            i += 1\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    \n",
    "    return numerical_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>location_url</th>\n",
       "      <th>route_url</th>\n",
       "      <th>state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/the-hobbit/10...</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/accidents-wil...</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/super-slide/1...</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/whos-got-the-...</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.mountainproject.com/v/headwall/1058...</td>\n",
       "      <td>http://www.mountainproject.com/v/last-of-the-g...</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       location_url                                          route_url     state  latitude  longitude\n",
       "0           0  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/the-hobbit/10...  Oklahoma         0          0\n",
       "1           1  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/accidents-wil...  Oklahoma         0          0\n",
       "2           2  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/super-slide/1...  Oklahoma         0          0\n",
       "3           3  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/whos-got-the-...  Oklahoma         0          0\n",
       "4           4  http://www.mountainproject.com/v/headwall/1058...  http://www.mountainproject.com/v/last-of-the-g...  Oklahoma         0          0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize the latitude/longitude columns\n",
    "df_routes['latitude'] = 0\n",
    "df_routes['longitude'] = 0\n",
    "df_routes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4786"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To lower run time, we wish to loop over only the unique locations (some routes share the same location.)\n",
    "len(df_routes['location_url'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This loop takes a long time to run and produces the coordinates. The length of the run is due to the wait times per\n",
    "#url scraped as the hierarchy of areas is climbed.\n",
    "for location in df_routes['location_url'].unique():\n",
    "    num_coords = get_location(location)\n",
    "    df_routes.loc[df_routes['location_url'] == location,'longitude'] = num_coords[1]\n",
    "    df_routes.loc[df_routes['location_url'] == location,'latitude'] = num_coords[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = df_routes[df_routes['latitude'] == 0]\n",
    "len(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All routes now have an associated coordinate, as shown by having NO routes where the default value of 0 was maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_url</th>\n",
       "      <th>route_url</th>\n",
       "      <th>state</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/the-hobbit/10...</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>34.8913</td>\n",
       "      <td>-99.3014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/accidents-wil...</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>34.8913</td>\n",
       "      <td>-99.3014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/super-slide/1...</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>34.8913</td>\n",
       "      <td>-99.3014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.mountainproject.com/v/sea-of-scream...</td>\n",
       "      <td>http://www.mountainproject.com/v/whos-got-the-...</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>34.8913</td>\n",
       "      <td>-99.3014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.mountainproject.com/v/headwall/1058...</td>\n",
       "      <td>http://www.mountainproject.com/v/last-of-the-g...</td>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>34.8913</td>\n",
       "      <td>-99.3014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        location_url                                          route_url     state  latitude  longitude\n",
       "0  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/the-hobbit/10...  Oklahoma   34.8913   -99.3014\n",
       "1  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/accidents-wil...  Oklahoma   34.8913   -99.3014\n",
       "2  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/super-slide/1...  Oklahoma   34.8913   -99.3014\n",
       "3  http://www.mountainproject.com/v/sea-of-scream...  http://www.mountainproject.com/v/whos-got-the-...  Oklahoma   34.8913   -99.3014\n",
       "4  http://www.mountainproject.com/v/headwall/1058...  http://www.mountainproject.com/v/last-of-the-g...  Oklahoma   34.8913   -99.3014"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_routes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finally, all the data is saved to a csv so the scrapping need not be run again.\n",
    "df_routes.to_csv(\"df_routes_lat_long.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
